from transformers import AutoModelForAudioClassification, AutoFeatureExtractor
import torch
import numpy as np
import librosa
import json
from datetime import datetime, timedelta
import os
import google.generativeai as genai

# åˆå§‹åŒ– Gemini æ¨¡å‹ï¼ˆé¿å…å¾ªç’°å°å…¥ï¼‰
genai.configure(api_key="AIzaSyBwbqy85wGVIN2idVvAmkL9ecnqwo-bDdc")
model = genai.GenerativeModel("gemini-2.0-flash")

def safe_generate(prompt):
    """å®‰å…¨ç”Ÿæˆæ–‡å­—å…§å®¹"""
    try:
        return model.generate_content(prompt).text.strip()
    except Exception as e:
        print(f"Gemini API éŒ¯èª¤: {e}")
        return "ä¸­æ€§"

# åˆå§‹åŒ–èªéŸ³æƒ…ç·’è¾¨è­˜æ¨¡å‹
model_id = "firdhokk/speech-emotion-recognition-with-openai-whisper-large-v3"
audio_model = AutoModelForAudioClassification.from_pretrained(model_id)
feature_extractor = AutoFeatureExtractor.from_pretrained(model_id, do_normalize=True)
id2label = audio_model.config.id2label

# æƒ…ç·’æ˜ å°„å’Œæ•¸å€¼åŒ–
EMOTION_MAPPING = {
    # èªéŸ³æƒ…ç·’æ˜ å°„åˆ°çµ±ä¸€æƒ…ç·’
    "angry": "ç”Ÿæ°£",
    "disgust": "ç”Ÿæ°£",  # å­æƒ¡æ­¸é¡ç‚ºç”Ÿæ°£
    "fearful": "æ‚²å‚·",  # ææ‡¼æ­¸é¡ç‚ºæ‚²å‚·
    "happy": "å¿«æ¨‚",
    "neutral": "ä¸­æ€§",
    "sad": "æ‚²å‚·",
    "surprised": "å¿«æ¨‚"  # é©šè¨æ­¸é¡ç‚ºå¿«æ¨‚ï¼ˆæ­£é¢æƒ…ç·’ï¼‰
}

# æƒ…ç·’æ•¸å€¼åŒ–ï¼ˆç”¨æ–¼æŠ˜ç·šåœ–ï¼Œæ•¸å€¼è¶Šé«˜ä»£è¡¨æƒ…ç·’è¶Šæ­£é¢ï¼‰
EMOTION_VALUES = {
    "å¿«æ¨‚": 3,
    "ä¸­æ€§": 2,
    "æ‚²å‚·": 1,
    "ç”Ÿæ°£": 0
}

# åå‘æ˜ å°„ï¼ˆæ•¸å€¼åˆ°æƒ…ç·’ï¼‰
VALUE_TO_EMOTION = {v: k for k, v in EMOTION_VALUES.items()}

# æ•¸æ“šæ–‡ä»¶è·¯å¾‘
WEEKLY_STATS_FILE = "weekly_emotion_stats.json"
DAILY_EMOTIONS_FILE = "daily_emotions.json"

def get_emotion_value(emotion):
    """å°‡æƒ…ç·’è½‰æ›ç‚ºæ•¸å€¼ï¼ˆç”¨æ–¼åœ–è¡¨é¡¯ç¤ºï¼‰"""
    return EMOTION_VALUES.get(emotion, 2)  # é è¨­ç‚ºä¸­æ€§(2)

def map_audio_emotion_to_unified(audio_emotion):
    """å°‡èªéŸ³æƒ…ç·’æ˜ å°„åˆ°çµ±ä¸€çš„4ç¨®æƒ…ç·’"""
    return EMOTION_MAPPING.get(audio_emotion, "ä¸­æ€§")

def detect_text_emotion(text):
    """
    åŸºæ–¼æ–‡å­—å…§å®¹é€²è¡Œæƒ…ç·’è¾¨è­˜ã€‚
    """
    prompt = f"""
    ä½ æ˜¯ä¸€å€‹æƒ…ç·’åˆ†æåŠ©æ‰‹ï¼Œè«‹å¾ä»¥ä¸‹å¥å­ä¸­åˆ¤æ–·ä½¿ç”¨è€…çš„æƒ…ç·’ï¼Œä¸¦åªå›è¦†ã€Œå¿«æ¨‚ã€ã€ã€Œæ‚²å‚·ã€ã€ã€Œç”Ÿæ°£ã€æˆ–ã€Œä¸­æ€§ã€å…¶ä¸­ä¸€ç¨®ï¼Œä¸è¦åŠ ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
    å¥å­ï¼šã€Œ{text}ã€
    """
    emotion = safe_generate(prompt)
    if emotion not in ["å¿«æ¨‚", "æ‚²å‚·", "ç”Ÿæ°£", "ä¸­æ€§"]:
        return "ä¸­æ€§"
    return emotion

def detect_audio_emotion(audio_path, max_duration=30.0):
    """
    åŸºæ–¼èªéŸ³æª”æ¡ˆé€²è¡Œæƒ…ç·’è¾¨è­˜ã€‚
    """
    try:
        # æª¢æŸ¥æª”æ¡ˆæ˜¯å¦å­˜åœ¨ (ç”¨æ–¼æ–‡å­—æ¸¬è©¦æ¨¡å¼)
        if not os.path.exists(audio_path):
            print(f"éŸ³æª”ä¸å­˜åœ¨ï¼Œè¿”å›ä¸­æ€§æƒ…ç·’ï¼š{audio_path}")
            return "ä¸­æ€§"
            
        # è¼‰å…¥éŸ³è¨Šä¸¦é€²è¡Œé•·åº¦è£œé½Šæˆ–è£å‰ª
        audio_array, _ = librosa.load(audio_path, sr=feature_extractor.sampling_rate)
        max_len = int(feature_extractor.sampling_rate * max_duration)
        if len(audio_array) > max_len:
            audio_array = audio_array[:max_len]
        else:
            audio_array = np.pad(audio_array, (0, max_len - len(audio_array)))

        # æå–ç‰¹å¾µ
        inputs = feature_extractor(audio_array, sampling_rate=feature_extractor.sampling_rate, return_tensors="pt")
        with torch.no_grad():
            logits = audio_model(**inputs).logits
        predicted_id = torch.argmax(logits, dim=-1).item()
        raw_emotion = id2label[predicted_id]
        # å°‡èªéŸ³æƒ…ç·’æ˜ å°„åˆ°çµ±ä¸€çš„4ç¨®æƒ…ç·’
        unified_emotion = map_audio_emotion_to_unified(raw_emotion)
        return unified_emotion
    except Exception as e:
        print(f"èªéŸ³æƒ…ç·’è¾¨è­˜å¤±æ•—ï¼š{e}")
        return "ä¸­æ€§"

def fuse_emotions(text_emotion, text_confidence=None, audio_emotion=None, audio_confidence=None, facial_emotion=None, facial_confidence=None):
    """
    å°‡æ–‡å­—ã€èªéŸ³å’Œè¡¨æƒ…çš„æƒ…ç·’æ¨™ç±¤èˆ‡ä¿¡å¿ƒåˆ†æ•¸é€²è¡ŒåŠ æ¬Šèåˆã€‚
    æ”¯æ´éƒ¨åˆ†æ¨¡æ…‹ç¼ºå¤±çš„æƒ…æ³ã€‚
    """
    # å‹•æ…‹èª¿æ•´æ¬Šé‡ï¼ˆæ ¹æ“šå¯ç”¨çš„æ¨¡æ…‹ï¼‰
    available_modalities = []
    if text_emotion:
        available_modalities.append('text')
    if audio_emotion:
        available_modalities.append('audio') 
    if facial_emotion:
        available_modalities.append('facial')
    
    if not available_modalities:
        return "ä¸­æ€§", {"å¿«æ¨‚": 0, "æ‚²å‚·": 0, "ç”Ÿæ°£": 0, "ä¸­æ€§": 1.0}
    
    # æ ¹æ“šå¯ç”¨æ¨¡æ…‹æ•¸é‡å‹•æ…‹åˆ†é…æ¬Šé‡ï¼ˆæé«˜æ–‡å­—æ¬Šé‡ï¼‰
    if len(available_modalities) == 1:
        weights = {'text': 1.0, 'audio': 1.0, 'facial': 1.0}
    elif len(available_modalities) == 2:
        if 'facial' not in available_modalities:
            # æ–‡å­—+èªéŸ³ï¼šæ–‡å­— 70%ï¼ŒèªéŸ³ 30%
            weights = {'text': 0.7, 'audio': 0.3, 'facial': 0.0}
        else:
            # æ–‡å­—+è‡‰éƒ¨ï¼šæ–‡å­— 75%ï¼Œè‡‰éƒ¨ 25% | èªéŸ³+è‡‰éƒ¨ï¼šèªéŸ³ 60%ï¼Œè‡‰éƒ¨ 40%
            weights = {'text': 0.75, 'audio': 0.0, 'facial': 0.25} if 'text' in available_modalities else {'text': 0.0, 'audio': 0.6, 'facial': 0.4}
    else:  # ä¸‰ç¨®æ¨¡æ…‹éƒ½æœ‰
        # æ–‡å­—+èªéŸ³+è‡‰éƒ¨ï¼šæ–‡å­— 60%ï¼ŒèªéŸ³ 25%ï¼Œè‡‰éƒ¨ 15%
        weights = {'text': 0.6, 'audio': 0.25, 'facial': 0.15}

    # åˆå§‹åŒ–æƒ…ç·’ä¿¡å¿ƒåˆ†æ•¸
    emotions = ["å¿«æ¨‚", "æ‚²å‚·", "ç”Ÿæ°£", "ä¸­æ€§"]
    final_confidence = {emotion: 0 for emotion in emotions}

    # å»ºç«‹ä¿¡å¿ƒåˆ†æ•¸å­—å…¸ï¼ˆå¦‚æœæ²’æœ‰æä¾›ï¼Œå‰‡æ ¹æ“šæƒ…ç·’é¡å‹çµ¦äºˆé è¨­åˆ†æ•¸ï¼‰
    def create_confidence_dict(emotion):
        if emotion in emotions:
            conf_dict = {e: 0.1 for e in emotions}  # å…¶ä»–æƒ…ç·’çµ¦ä½åˆ†
            conf_dict[emotion] = 0.8  # ä¸»è¦æƒ…ç·’çµ¦é«˜åˆ†
            return conf_dict
        return {e: 0.25 for e in emotions}  # å¦‚æœæƒ…ç·’ç„¡æ•ˆï¼Œå¹³å‡åˆ†é…

    # è™•ç†æ–‡å­—æƒ…ç·’
    if text_emotion and 'text' in available_modalities:
        text_conf = text_confidence if text_confidence else create_confidence_dict(text_emotion)
        for emotion in emotions:
            final_confidence[emotion] += text_conf.get(emotion, 0) * weights['text']

    # è™•ç†èªéŸ³æƒ…ç·’  
    if audio_emotion and 'audio' in available_modalities:
        audio_conf = audio_confidence if audio_confidence else create_confidence_dict(audio_emotion)
        for emotion in emotions:
            final_confidence[emotion] += audio_conf.get(emotion, 0) * weights['audio']

    # è™•ç†è‡‰éƒ¨æƒ…ç·’
    if facial_emotion and 'facial' in available_modalities:
        facial_conf = facial_confidence if facial_confidence else create_confidence_dict(facial_emotion)
        for emotion in emotions:
            final_confidence[emotion] += facial_conf.get(emotion, 0) * weights['facial']

    # æ¨™æº–åŒ–ä¿¡å¿ƒåˆ†æ•¸
    total_weight = sum(weights[mod] for mod in available_modalities)
    if total_weight > 0:
        for emotion in final_confidence:
            final_confidence[emotion] /= total_weight

    # ç²å–æœ€çµ‚æƒ…ç·’æ¨™ç±¤
    final_emotion = max(final_confidence, key=final_confidence.get)
    
    # å››æ¨äº”å…¥ä¿¡å¿ƒåˆ†æ•¸
    final_confidence = {k: round(v, 3) for k, v in final_confidence.items()}
    
    print(f"ğŸ”€ æƒ…ç·’èåˆçµæœ: {final_emotion} | æ¨¡æ…‹: {'+'.join(available_modalities)}")
    return final_emotion, final_confidence

def save_emotion_data(daily_emotions, weekly_emotion_stats):
    """å„²å­˜æƒ…ç·’æ•¸æ“šåˆ°æ–‡ä»¶"""
    try:
        # å„²å­˜æ¯æ—¥æƒ…ç·’æ•¸æ“š
        with open(DAILY_EMOTIONS_FILE, "w", encoding="utf-8") as f:
            json.dump(daily_emotions, f, ensure_ascii=False, indent=4)

        # å„²å­˜æ¯é€±æƒ…ç·’çµ±è¨ˆæ•¸æ“š
        with open(WEEKLY_STATS_FILE, "w", encoding="utf-8") as f:
            json.dump(weekly_emotion_stats, f, ensure_ascii=False, indent=4)
    except Exception as e:
        print(f"å„²å­˜æƒ…ç·’æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

def load_emotion_data():
    """å¾æ–‡ä»¶è¼‰å…¥æƒ…ç·’æ•¸æ“š"""
    daily_emotions = {}
    weekly_emotion_stats = {}
    try:
        # è¼‰å…¥æ¯æ—¥æƒ…ç·’æ•¸æ“š
        if os.path.exists(DAILY_EMOTIONS_FILE):
            with open(DAILY_EMOTIONS_FILE, "r", encoding="utf-8") as f:
                daily_emotions = json.load(f)

        # è¼‰å…¥æ¯é€±æƒ…ç·’çµ±è¨ˆæ•¸æ“š
        if os.path.exists(WEEKLY_STATS_FILE):
            with open(WEEKLY_STATS_FILE, "r", encoding="utf-8") as f:
                weekly_emotion_stats = json.load(f)
    except Exception as e:
        print(f"è¼‰å…¥æƒ…ç·’æ•¸æ“šæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
    return daily_emotions, weekly_emotion_stats

def update_weekly_stats(emotion_label, timestamp, weekly_emotion_stats):
    """
    æ›´æ–°æ¯é€±æƒ…ç·’çµ±è¨ˆæ•¸æ“šã€‚
    """
    try:
        # ç²å–ç•¶å‰æ™‚é–“
        now = datetime.strptime(timestamp, "%Y-%m-%d %H:%M:%S")
        start_of_week = now - timedelta(days=now.weekday())  # æœ¬é€±é–‹å§‹æ™‚é–“
        end_of_week = start_of_week + timedelta(days=6)  # æœ¬é€±çµæŸæ™‚é–“

        # åˆå§‹åŒ–æœ¬é€±æƒ…ç·’çµ±è¨ˆ
        if str(start_of_week.date()) not in weekly_emotion_stats:
            weekly_emotion_stats[str(start_of_week.date())] = {
                "å¿«æ¨‚": 0,
                "æ‚²å‚·": 0,
                "ç”Ÿæ°£": 0,
                "ä¸­æ€§": 0
            }

        # æ›´æ–°æƒ…ç·’è¨ˆæ•¸
        weekly_emotion_stats[str(start_of_week.date())][emotion_label] += 1

        # å„²å­˜æ•¸æ“š
        save_emotion_data({}, weekly_emotion_stats)
    except Exception as e:
        print(f"æ›´æ–°æ¯é€±çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

def get_weekly_emotion_stats(weekly_emotion_stats):
    """
    ç²å–æ¯é€±æƒ…ç·’çµ±è¨ˆæ•¸æ“šã€‚
    """
    try:
        # è¨ˆç®—æ¯é€±æƒ…ç·’æ¯”ä¾‹
        for date, stats in weekly_emotion_stats.items():
            total = sum(stats.values())
            if total > 0:
                for emotion in stats:
                    stats[emotion] = round(stats[emotion] / total, 4)  # æ¯”ä¾‹ä¿ç•™å››ä½å°æ•¸
    except Exception as e:
        print(f"è¨ˆç®—æ¯é€±æƒ…ç·’çµ±è¨ˆæ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")

def record_daily_emotion(emotion, confidence_score=None):
    """è¨˜éŒ„æ¯æ—¥æƒ…ç·’æ•¸æ“š"""
    today = datetime.now().strftime("%Y-%m-%d")
    emotion_value = get_emotion_value(emotion)
    
    # è¼‰å…¥ç¾æœ‰æ•¸æ“š
    if os.path.exists(DAILY_EMOTIONS_FILE):
        with open(DAILY_EMOTIONS_FILE, 'r', encoding='utf-8') as f:
            daily_data = json.load(f)
    else:
        daily_data = {}
    
    # åˆå§‹åŒ–ä»Šæ—¥æ•¸æ“š
    if today not in daily_data:
        daily_data[today] = {
            "emotions": [],
            "values": [],
            "avg_value": 0,
            "dominant_emotion": "ä¸­æ€§"
        }
    
    # è¨˜éŒ„æƒ…ç·’
    daily_data[today]["emotions"].append(emotion)
    daily_data[today]["values"].append(emotion_value)
    
    # è¨ˆç®—ç•¶æ—¥å¹³å‡æƒ…ç·’å€¼
    values = daily_data[today]["values"]
    daily_data[today]["avg_value"] = sum(values) / len(values)
    
    # è¨ˆç®—ç•¶æ—¥ä¸»è¦æƒ…ç·’
    emotion_counts = {}
    for e in daily_data[today]["emotions"]:
        emotion_counts[e] = emotion_counts.get(e, 0) + 1
    daily_data[today]["dominant_emotion"] = max(emotion_counts, key=emotion_counts.get)
    
    # ä¿å­˜æ•¸æ“š
    with open(DAILY_EMOTIONS_FILE, 'w', encoding='utf-8') as f:
        json.dump(daily_data, f, ensure_ascii=False, indent=2)
    
    return daily_data[today]

def calculate_weekly_stats():
    """è¨ˆç®—é€±çµ±è¨ˆæ•¸æ“šï¼ˆæ¯æ™š9é»èª¿ç”¨ï¼‰"""
    now = datetime.now()
    
    # è¨ˆç®—æœ¬é€±çš„æ—¥æœŸç¯„åœï¼ˆé€±ä¸€åˆ°é€±æ—¥ï¼‰
    days_since_monday = now.weekday()
    monday = now - timedelta(days=days_since_monday)
    week_start = monday.strftime("%Y-%m-%d")
    week_end = (monday + timedelta(days=6)).strftime("%Y-%m-%d")
    week_key = f"{monday.strftime('%Y-W%U')}"  # å¹´ä»½-ç¬¬å¹¾é€±
    
    # è¼‰å…¥æ¯æ—¥æ•¸æ“š
    if not os.path.exists(DAILY_EMOTIONS_FILE):
        return None
        
    with open(DAILY_EMOTIONS_FILE, 'r', encoding='utf-8') as f:
        daily_data = json.load(f)
    
    # æ”¶é›†æœ¬é€±æ•¸æ“š
    week_values = []
    week_emotions = []
    daily_averages = []
    
    current_date = monday
    for i in range(7):  # é€±ä¸€åˆ°é€±æ—¥
        date_str = current_date.strftime("%Y-%m-%d")
        if date_str in daily_data:
            daily_avg = daily_data[date_str]["avg_value"]
            dominant_emotion = daily_data[date_str]["dominant_emotion"]
            daily_averages.append(daily_avg)
            week_values.extend(daily_data[date_str]["values"])
            week_emotions.extend(daily_data[date_str]["emotions"])
        else:
            daily_averages.append(2)  # æ²’æœ‰æ•¸æ“šçš„æ—¥å­é è¨­ç‚ºä¸­æ€§
            
        current_date += timedelta(days=1)
    
    # è¨ˆç®—é€±çµ±è¨ˆ
    week_stats = {
        "week": week_key,
        "week_start": week_start,
        "week_end": week_end,
        "daily_averages": daily_averages,  # 7å¤©çš„æ¯æ—¥å¹³å‡å€¼
        "week_average": sum(daily_averages) / len(daily_averages),
        "total_records": len(week_values),
        "emotion_distribution": {},
        "timestamp": now.strftime("%Y-%m-%d %H:%M:%S")
    }
    
    # è¨ˆç®—æƒ…ç·’åˆ†å¸ƒ
    for emotion in week_emotions:
        week_stats["emotion_distribution"][emotion] = week_stats["emotion_distribution"].get(emotion, 0) + 1
    
    # è¼‰å…¥é€±çµ±è¨ˆæ–‡ä»¶
    if os.path.exists(WEEKLY_STATS_FILE):
        with open(WEEKLY_STATS_FILE, 'r', encoding='utf-8') as f:
            weekly_data = json.load(f)
    else:
        weekly_data = []
    
    # æ›´æ–°æˆ–æ–°å¢æœ¬é€±æ•¸æ“š
    week_found = False
    for i, week_data in enumerate(weekly_data):
        if week_data["week"] == week_key:
            weekly_data[i] = week_stats
            week_found = True
            break
    
    if not week_found:
        weekly_data.append(week_stats)
    
    # ä¿å­˜é€±çµ±è¨ˆ
    with open(WEEKLY_STATS_FILE, 'w', encoding='utf-8') as f:
        json.dump(weekly_data, f, ensure_ascii=False, indent=2)
    
    return week_stats

def get_chart_data(weeks=12):
    """ç²å–å‰ç«¯åœ–è¡¨æ‰€éœ€çš„æ•¸æ“š"""
    if not os.path.exists(WEEKLY_STATS_FILE):
        return {"weeks": [], "values": [], "emotions": []}
    
    with open(WEEKLY_STATS_FILE, 'r', encoding='utf-8') as f:
        weekly_data = json.load(f)
    
    # å–æœ€è¿‘æŒ‡å®šé€±æ•¸çš„æ•¸æ“š
    recent_data = weekly_data[-weeks:] if len(weekly_data) > weeks else weekly_data
    
    chart_data = {
        "weeks": [data["week"] for data in recent_data],
        "values": [data["week_average"] for data in recent_data],
        "emotions": [VALUE_TO_EMOTION.get(round(data["week_average"]), "ä¸­æ€§") for data in recent_data],
        "daily_details": [data["daily_averages"] for data in recent_data]
    }
    
    return chart_data

def schedule_weekly_update():
    """å®‰æ’æ¯æ™š9é»çš„é€±çµ±è¨ˆæ›´æ–°"""
    import schedule
    
    def update_job():
        print(f"[{datetime.now()}] åŸ·è¡Œé€±çµ±è¨ˆæ›´æ–°...")
        try:
            stats = calculate_weekly_stats()
            if stats:
                print(f"æœ¬é€±å¹³å‡æƒ…ç·’å€¼ï¼š{stats['week_average']:.2f}")
                print(f"æœ¬é€±ç¸½è¨˜éŒ„æ•¸ï¼š{stats['total_records']}")
            else:
                print("æš«ç„¡æ•¸æ“šå¯çµ±è¨ˆ")
        except Exception as e:
            print(f"é€±çµ±è¨ˆæ›´æ–°å¤±æ•—ï¼š{e}")
    
    # æ¯å¤©æ™šä¸Š9é»åŸ·è¡Œ
    schedule.every().day.at("21:00").do(update_job)
    print(" é€±çµ±è¨ˆå®šæ™‚ä»»å‹™å·²è¨­ç½®ï¼ˆæ¯æ™š21:00åŸ·è¡Œï¼‰")
    return schedule

# è‡‰éƒ¨è¾¨è­˜ç›¸é—œå°å…¥ (æ¢ä»¶å¼å°å…¥ï¼Œé¿å…é–‹ç™¼ç’°å¢ƒç¼ºå°‘å¥—ä»¶)
FACIAL_RECOGNITION_AVAILABLE = False
try:
    import cv2
    from fer import FER
    FACIAL_RECOGNITION_AVAILABLE = True
    print(" è‡‰éƒ¨è¾¨è­˜æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
except ImportError as e:
    print(f" è‡‰éƒ¨è¾¨è­˜æ¨¡çµ„æœªå®‰è£: {e}")
    print("  å°‡ä½¿ç”¨æ¨¡æ“¬æ¨¡å¼é€²è¡Œé–‹ç™¼æ¸¬è©¦")
    print("  éƒ¨ç½²æ™‚è«‹å®‰è£: pip install opencv-python fer")

# è¨­å®šæ¨¡å¼ï¼šé–‹ç™¼æ¨¡å¼ä½¿ç”¨æ¨¡æ“¬æ•¸æ“šï¼Œç”Ÿç”¢æ¨¡å¼ä½¿ç”¨çœŸå¯¦æ”å½±æ©Ÿ
SIMULATION_MODE = not FACIAL_RECOGNITION_AVAILABLE  # å¦‚æœå¥—ä»¶æœªå®‰è£ï¼Œè‡ªå‹•å•Ÿç”¨æ¨¡æ“¬æ¨¡å¼
CAMERA_DEVICE_ID = 0  # é è¨­æ”å½±æ©ŸID

# è‡‰éƒ¨æƒ…ç·’è¾¨è­˜ç›¸é—œè¨­å®š
FACIAL_EMOTION_MODEL = "emotion_model.onnx"  # é è¨­ä½¿ç”¨ ONNX æ ¼å¼çš„æ¨¡å‹
FACIAL_EMOTION_LABELS = ["å¿«æ¨‚", "æ‚²å‚·", "ç”Ÿæ°£", "ä¸­æ€§"]  # é è¨­æƒ…ç·’æ¨™ç±¤

# è¼‰å…¥è‡‰éƒ¨æƒ…ç·’è¾¨è­˜æ¨¡å‹
def load_facial_emotion_model(model_path=FACIAL_EMOTION_MODEL):
    """è¼‰å…¥è‡‰éƒ¨æƒ…ç·’è¾¨è­˜æ¨¡å‹ï¼ˆONNX æ ¼å¼ï¼‰"""
    import onnx
    from onnx_tf.backend import prepare

    # è®€å– ONNX æ¨¡å‹
    onnx_model = onnx.load(model_path)
    # è½‰æ›ç‚º TensorFlow æ¨¡å‹
    tf_rep = prepare(onnx_model)
    return tf_rep

# åµæ¸¬è‡‰éƒ¨æƒ…ç·’
def detect_facial_emotion(frame, model, labels=FACIAL_EMOTION_LABELS):
    """
    åµæ¸¬å–®å¼µå½±åƒä¸­çš„è‡‰éƒ¨æƒ…ç·’ã€‚
    """
    try:
        # è½‰æ›é¡è‰²é€šé“ BGR -> RGB
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        # åµæ¸¬è‡‰éƒ¨å€åŸŸ
        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + "haarcascade_frontalface_default.xml")
        faces = face_cascade.detectMultiScale(frame_rgb, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))

        if len(faces) == 0:
            return "ä¸­æ€§", 0.0  # æœªåµæ¸¬åˆ°è‡‰éƒ¨ï¼Œå›å‚³ä¸­æ€§æƒ…ç·’

        # å–ç¬¬ä¸€å€‹åµæ¸¬åˆ°çš„è‡‰éƒ¨
        (x, y, w, h) = faces[0]
        face_roi = frame_rgb[y:y+h, x:x+w]

        # èª¿æ•´å½±åƒå¤§å°ä»¥ç¬¦åˆæ¨¡å‹è¼¸å…¥
        face_roi_resized = cv2.resize(face_roi, (48, 48))
        face_roi_normalized = face_roi_resized / 255.0  # æ­£è¦åŒ–åˆ° [0, 1] ç¯„åœ
        face_roi_reshaped = np.reshape(face_roi_normalized, (1, 48, 48, 3))  # èª¿æ•´å½¢ç‹€

        # é€²è¡Œé æ¸¬
        predictions = model.run(None, {"input": face_roi_reshaped.astype(np.float32)})
        scores = predictions[0][0]

        # ç²å–æœ€é«˜åˆ†æ•¸çš„æƒ…ç·’æ¨™ç±¤
        max_index = np.argmax(scores)
        emotion = labels[max_index]
        confidence = scores[max_index]

        return emotion, confidence
    except Exception as e:
        print(f"è‡‰éƒ¨æƒ…ç·’è¾¨è­˜å¤±æ•—ï¼š{e}")
        return "ä¸­æ€§", 0.0

# æ¨¡æ“¬è‡‰éƒ¨æƒ…ç·’è¾¨è­˜ï¼ˆç”¨æ–¼é–‹ç™¼æ¸¬è©¦ï¼‰
def simulate_facial_emotion():
    """
    æ¨¡æ“¬è‡‰éƒ¨æƒ…ç·’è¾¨è­˜çµæœã€‚
    """
    import random
    emotion = random.choice(FACIAL_EMOTION_LABELS)
    confidence = random.uniform(0.5, 1.0)
    return emotion, confidence

def detect_facial_emotion_simulation():
    """æ¨¡æ“¬è‡‰éƒ¨æƒ…ç·’è¾¨è­˜ï¼ˆé–‹ç™¼æ¸¬è©¦ç”¨ï¼‰"""
    import random
    
    # æ¨¡æ“¬ä¸åŒæƒ…ç·’çš„æ©Ÿç‡åˆ†å¸ƒ
    emotions_prob = {
        "å¿«æ¨‚": 0.3,
        "ä¸­æ€§": 0.4, 
        "æ‚²å‚·": 0.2,
        "ç”Ÿæ°£": 0.1
    }
    
    # éš¨æ©Ÿé¸æ“‡æƒ…ç·’ï¼Œæ¨¡æ“¬çœŸå¯¦æƒ…æ³
    emotion = random.choices(
        list(emotions_prob.keys()), 
        weights=list(emotions_prob.values())
    )[0]
    
    # æ¨¡æ“¬ä¿¡å¿ƒåˆ†æ•¸ (0.6-0.95 ä¹‹é–“)
    confidence = round(random.uniform(0.6, 0.95), 3)
    
    print(f" [æ¨¡æ“¬] è‡‰éƒ¨æƒ…ç·’: {emotion} (ä¿¡å¿ƒåº¦: {confidence})")
    return emotion, confidence

def detect_facial_emotion_real(capture_duration=3.0):
    """çœŸå¯¦è‡‰éƒ¨æƒ…ç·’è¾¨è­˜ï¼ˆç”Ÿç”¢ç’°å¢ƒç”¨ï¼‰"""
    if not FACIAL_RECOGNITION_AVAILABLE:
        print(" è‡‰éƒ¨è¾¨è­˜å¥—ä»¶æœªå®‰è£ï¼Œè«‹å®‰è£: pip install opencv-python fer")
        return detect_facial_emotion_simulation()
    
    try:
        # åˆå§‹åŒ–è‡‰éƒ¨æƒ…ç·’æª¢æ¸¬å™¨
        detector = FER()
        cap = cv2.VideoCapture(CAMERA_DEVICE_ID)
        
        if not cap.isOpened():
            print(f" ç„¡æ³•é–‹å•Ÿæ”å½±æ©Ÿ (Device ID: {CAMERA_DEVICE_ID})")
            return detect_facial_emotion_simulation()
        
        print(f" é–‹å§‹è‡‰éƒ¨æƒ…ç·’æ•æ‰ ({capture_duration}ç§’)...")
        
        emotion_results = []
        frame_count = 0
        start_time = cv2.getTickCount()
        
        while True:
            ret, frame = cap.read()
            if not ret:
                break
                
            # æ¯éš”å¹¾å¹€æª¢æ¸¬ä¸€æ¬¡ï¼ˆæå‡æ€§èƒ½ï¼‰
            if frame_count % 5 == 0:
                emotions = detector.detect_emotions(frame)
                if emotions:
                    # å–ç¬¬ä¸€å€‹æª¢æ¸¬åˆ°çš„è‡‰éƒ¨
                    emotion_scores = emotions[0]['emotions']
                    dominant_emotion = max(emotion_scores, key=emotion_scores.get)
                    confidence = emotion_scores[dominant_emotion]
                    
                    # æ˜ å°„åˆ°çµ±ä¸€æƒ…ç·’æ ¼å¼
                    unified_emotion = map_facial_emotion_to_unified(dominant_emotion)
                    emotion_results.append((unified_emotion, confidence))
            
            frame_count += 1
            
            # æª¢æŸ¥æ˜¯å¦é”åˆ°æŒ‡å®šæ™‚é–“
            elapsed_time = (cv2.getTickCount() - start_time) / cv2.getTickFrequency()
            if elapsed_time >= capture_duration:
                break
        
        cap.release()
        cv2.destroyAllWindows()
        
        if not emotion_results:
            print(" æœªæª¢æ¸¬åˆ°è‡‰éƒ¨ï¼Œä½¿ç”¨é è¨­æƒ…ç·’")
            return "ä¸­æ€§", 0.5
        
        # è¨ˆç®—å¹³å‡æƒ…ç·’ï¼ˆåŠ æ¬Šå¹³å‡ï¼‰
        emotion_weights = {}
        total_confidence = 0
        
        for emotion, confidence in emotion_results:
            if emotion not in emotion_weights:
                emotion_weights[emotion] = 0
            emotion_weights[emotion] += confidence
            total_confidence += confidence
        
        # æ‰¾å‡ºæœ€ä¸»è¦çš„æƒ…ç·’
        dominant_emotion = max(emotion_weights, key=emotion_weights.get)
        avg_confidence = round(total_confidence / len(emotion_results), 3)
        
        print(f" è‡‰éƒ¨æƒ…ç·’æª¢æ¸¬å®Œæˆ: {dominant_emotion} (å¹³å‡ä¿¡å¿ƒåº¦: {avg_confidence})")
        return dominant_emotion, avg_confidence
        
    except Exception as e:
        print(f" è‡‰éƒ¨æƒ…ç·’è¾¨è­˜å¤±æ•—: {e}")
        return detect_facial_emotion_simulation()

def map_facial_emotion_to_unified(facial_emotion):
    """å°‡ FER æª¢æ¸¬çš„æƒ…ç·’æ˜ å°„åˆ°çµ±ä¸€æ ¼å¼"""
    facial_mapping = {
        # FER æª¢æ¸¬çš„æƒ…ç·’é¡å‹æ˜ å°„
        "happy": "å¿«æ¨‚",
        "sad": "æ‚²å‚·", 
        "angry": "ç”Ÿæ°£",
        "fear": "æ‚²å‚·",      # ææ‡¼æ­¸é¡ç‚ºæ‚²å‚·
        "surprise": "å¿«æ¨‚",  # é©šè¨æ­¸é¡ç‚ºå¿«æ¨‚
        "disgust": "ç”Ÿæ°£",   # å­æƒ¡æ­¸é¡ç‚ºç”Ÿæ°£
        "neutral": "ä¸­æ€§"
    }
    return facial_mapping.get(facial_emotion.lower(), "ä¸­æ€§")

def detect_facial_emotion():
    """çµ±ä¸€çš„è‡‰éƒ¨æƒ…ç·’è¾¨è­˜æ¥å£"""
    if SIMULATION_MODE:
        return detect_facial_emotion_simulation()
    else:
        return detect_facial_emotion_real()

def set_facial_recognition_mode(simulation=False, camera_id=0):
    """è¨­å®šè‡‰éƒ¨è¾¨è­˜æ¨¡å¼"""
    global SIMULATION_MODE, CAMERA_DEVICE_ID
    SIMULATION_MODE = simulation
    CAMERA_DEVICE_ID = camera_id
    
    mode_text = "æ¨¡æ“¬æ¨¡å¼" if simulation else f"çœŸå¯¦æ¨¡å¼ (æ”å½±æ©Ÿ ID: {camera_id})"
    print(f" è‡‰éƒ¨è¾¨è­˜è¨­å®šç‚º: {mode_text}")

def multi_modal_emotion_detection(text, audio_path=None, enable_facial=False, capture_duration=3.0):
    """
    å¤šæ¨¡æ…‹æƒ…ç·’è¾¨è­˜çµ±ä¸€æ¥å£
    
    Args:
        text: è¦åˆ†æçš„æ–‡å­—
        audio_path: èªéŸ³æª”æ¡ˆè·¯å¾‘ï¼ˆå¯é¸ï¼‰
        enable_facial: æ˜¯å¦å•Ÿç”¨è‡‰éƒ¨è¾¨è­˜
        capture_duration: è‡‰éƒ¨æ•æ‰æŒçºŒæ™‚é–“ï¼ˆç§’ï¼‰
    
    Returns:
        tuple: (æœ€çµ‚æƒ…ç·’, è©³ç´°çµæœå­—å…¸)
    """
    results = {
        "text_emotion": None,
        "audio_emotion": None, 
        "facial_emotion": None,
        "final_emotion": None,
        "confidence_scores": {},
        "modalities_used": []
    }
    
    print(f" é–‹å§‹å¤šæ¨¡æ…‹æƒ…ç·’åˆ†æ...")
    
    # 1. æ–‡å­—æƒ…ç·’è¾¨è­˜
    if text and text.strip():
        results["text_emotion"] = detect_text_emotion(text)
        results["modalities_used"].append("æ–‡å­—")
        print(f" æ–‡å­—æƒ…ç·’: {results['text_emotion']}")
    
    # 2. èªéŸ³æƒ…ç·’è¾¨è­˜
    if audio_path and os.path.exists(audio_path):
        results["audio_emotion"] = detect_audio_emotion(audio_path)
        results["modalities_used"].append("èªéŸ³")
        print(f" èªéŸ³æƒ…ç·’: {results['audio_emotion']}")
    
    # 3. è‡‰éƒ¨æƒ…ç·’è¾¨è­˜
    if enable_facial:
        facial_emotion, facial_confidence = detect_facial_emotion()
        results["facial_emotion"] = facial_emotion
        results["modalities_used"].append("è‡‰éƒ¨")
        print(f" è‡‰éƒ¨æƒ…ç·’: {facial_emotion} (ä¿¡å¿ƒåº¦: {facial_confidence})")
    
    # 4. æƒ…ç·’èåˆ
    if len(results["modalities_used"]) > 1:
        # å¤šæ¨¡æ…‹èåˆ
        final_emotion, confidence_scores = fuse_emotions(
            text_emotion=results["text_emotion"],
            audio_emotion=results["audio_emotion"],
            facial_emotion=results["facial_emotion"]
        )
        results["final_emotion"] = final_emotion
        results["confidence_scores"] = confidence_scores
        print(f" èåˆå¾Œæƒ…ç·’: {final_emotion}")
    else:
        # å–®æ¨¡æ…‹çµæœ
        single_emotion = (results["text_emotion"] or 
                         results["audio_emotion"] or 
                         results["facial_emotion"] or 
                         "ä¸­æ€§")
        results["final_emotion"] = single_emotion
        results["confidence_scores"] = {single_emotion: 0.8, "å…¶ä»–": 0.2}
    
    print(f" åˆ†æå®Œæˆï¼Œä½¿ç”¨æ¨¡æ…‹: {'+'.join(results['modalities_used'])}")
    return results["final_emotion"], results

def emotion_analysis_demo():
    """æƒ…ç·’åˆ†æç³»çµ±æ¼”ç¤º"""
    print("=" * 50)
    print("å¤šæ¨¡æ…‹æƒ…ç·’åˆ†æç³»çµ±æ¼”ç¤º")
    print("=" * 50)
    
    # æ¸¬è©¦ä¸åŒæ¨¡æ…‹çµ„åˆ
    test_cases = [
        {
            "name": "ç´”æ–‡å­—æ¨¡å¼",
            "text": "ä»Šå¤©çœŸçš„å¾ˆé–‹å¿ƒï¼Œçµ‚æ–¼å®Œæˆå°ˆæ¡ˆäº†ï¼",
            "audio": None,
            "facial": False
        },
        {
            "name": "æ–‡å­—+èªéŸ³æ¨¡å¼", 
            "text": "æˆ‘è¦ºå¾—æœ‰é»ç´¯...",
            "audio": "audio_input.wav",  # å¦‚æœå­˜åœ¨çš„è©±
            "facial": False
        },
        {
            "name": "å…¨æ¨¡æ…‹æ¨¡å¼",
            "text": "ä½ å¥½å—ï¼Ÿ",
            "audio": "audio_input.wav",
            "facial": True  # æœƒä½¿ç”¨æ¨¡æ“¬æ¨¡å¼
        }
    ]
    
    for i, case in enumerate(test_cases, 1):
        print(f"\n æ¸¬è©¦ {i}: {case['name']}")
        print("-" * 30)
        
        final_emotion, details = multi_modal_emotion_detection(
            text=case["text"],
            audio_path=case["audio"],
            enable_facial=case["facial"]
        )
        
        print(f" æœ€çµ‚çµæœ: {final_emotion}")
        print(f" ä½¿ç”¨æ¨¡æ…‹: {', '.join(details['modalities_used'])}")
        
        if details["confidence_scores"]:
            print(" ä¿¡å¿ƒåˆ†æ•¸:")
            for emotion, score in details["confidence_scores"].items():
                print(f"   {emotion}: {score:.3f}")
    
    print("\n" + "=" * 50)
    print("æ¼”ç¤ºå®Œæˆ")

# å¦‚æœç›´æ¥åŸ·è¡Œæ­¤æª”æ¡ˆï¼Œé‹è¡Œæ¼”ç¤º
if __name__ == "__main__":
    emotion_analysis_demo()
